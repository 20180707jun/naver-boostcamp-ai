# 0910
- language 모델은 이제 우리 컴퓨터로는 1epoch도 돌릴 수 없다.
- 이제 language모델 논문은 읽지 않는다.

논문 트렌드는 부르기 쉽게 약자로 쓰여있고, 많은 의미들이 함축되어있다. 따라서 

첫번째 피규어가 전체 프레임에 대한 오버뷰를 의미한다. 따라서 제목을 ㅇ릭고 바로 figure로가는 것을 추천(많이 읽었다면)

코드가 있는지 체크

풀고자 하는 문제가 무엇인지
- 이러한

기존 paper들은 이 task를 어떻게 풀어왔는지
- related work가 굉장히 중요하다.
- 잘 쓴 논문들은 이러한 work에 대해서 설명을 정말 잘되어있다. 
- ex
  - bert가 나왔을 때 elmo를 언급
  - 이는 어떤 내용으로 논문이 쓰여져있는지 알 수 있다.
  - 기존의 문제들은 어떤 문제가 있다.
  - 그리고 나는 이걸 발전 시켰고, 어떤 점에서 발전 시킬 수 있었다.

논문을 결국에는 많이 읽어야한다.

논문을 읽을 때 어떤 부분이 해결이 되지 않는 부분은?
- 체크해두고 나중에 코드로 읽어보면 포인트를 잡기가 쉽다.

논문 내용과 내 생각을 짧게라도 정리해두기
- 나중에 생각이 나지 않음.

---
- 그림들에서 멀티헤드 어텐션, 마스크드 멀티 헤드 어텐션에서 뭔가 비슷하지만 다른 구조이겠구나라는 것을 알아야한다.
- scaled dot-product attention에서 q,k,v를 보고 굉장히 중요한 것이구나 하고 알 수 있다.
- self-attention이라고 하는데, 이 부분을 계산하는 부분이 면접에서 굉장히 많이나온다.(백지에서부터 쓸 수 있을 정도로 외워야한다.)
- 쿼리, 키, 벨류는 어텐션에서 나온 개념이다.
- 이렇게 되면 어텐션을 찾아보러 가야한다.
- self attention에서는 이들이 같아서 재미있는 부분이다.
- 나자신과 얼마나 비슷한지를 아는 것이 self attention에서 self의 개념이 된다.


질문 
뭔가를 막는다? 가 어떤 뜻...

key word
- Task agnostic, 
- cloze test


MT-DNN을 넣은 이유 (MT : multi task)
- bert는 하나에서 여러가지 task를 할 수 없다.
- MT-DNN은 여러가지 task를 할 수 있지 않을까? 하는 의문에서 시작.
- 모델도 그러한 다양한 task를 수행할 수 있지 않을까?

PLM 구현하려고한다면, transformer를 구현해야 한다.
- 유튜브 링크를 줄 것이다. 하버드는 틀렸다. 잘못되었다.