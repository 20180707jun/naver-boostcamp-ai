## data augmentation
### important Concepts in Optimization
- Generalization
- Under-fitting vs over-fitting
- Cross validation
- Bias-variance tradeoff
- Bootstrapping
- Bagging and boosting

### Generalization
![image](https://user-images.githubusercontent.com/50571795/129368683-6eb8eb33-55fc-4617-a5f7-1d2ff82e9a53.png)

### Underfitting vs Overfitting
![image](https://user-images.githubusercontent.com/50571795/129368770-28f13f4f-046c-4524-be29-e2719329f080.png)

### Cross-validation
![image](https://user-images.githubusercontent.com/50571795/129368871-0ae67b6a-d28c-4c35-a831-953e9bc81cdb.png)

### Bias and Variance

![image](https://user-images.githubusercontent.com/50571795/129369026-79665626-1f86-421f-b7af-47e2351c60d9.png)

![image](https://user-images.githubusercontent.com/50571795/129369073-baccbfc8-76d2-4d76-b266-073f00ae8f22.png)

### Bootstrapping
- Bagging(**B**oostrapping **agg**regat**ing**)
  - 여러 개의 모델을 병렬적으로 처리하여 voting을 통해서 결과를 냄
- Boosting
  - 여러 개의 약한 모델을 직렬로 연결하여 강한 모델을 만들어냄.

## Practical Gradient Descent Methods
- Stochastic gradient descent : 한 개의 샘플에 대해서 gradient를 계산하여 업데이트
- Mini-batch gradient descent : 한 부분집합에 대해서 gradient를 계산해서 업데이트 우리가 흔히 말하는 SGD
- Batch gradient descent : 전체 데이터에 대해서 업데이트. 느림. 

![image](https://user-images.githubusercontent.com/50571795/129373308-4887ffa8-9103-4644-bd94-9f6480e738eb.png)  
우리는 FlatMinimum에 도달하는 것이 좋다. generalize performance가 좋음.

**Gradient Descent**  
![image](https://user-images.githubusercontent.com/50571795/129373429-f11f221b-8289-449d-862b-620ca6e2ba79.png)
**Momentum**  
![image](https://user-images.githubusercontent.com/50571795/129373535-0bbc22e0-0e74-445f-b39d-9654e0375726.png)
**Nesterov Accelerated Gradient**  
![image](https://user-images.githubusercontent.com/50571795/129373631-f07a0cb3-d1f2-4072-941c-45983b648c5b.png)
**Adam**  
![image](https://user-images.githubusercontent.com/50571795/129373785-a4252811-e02d-469d-b1c3-f7b5082d14c7.png)

## Regularization
- Early stopping
- Parameter norm penalty
- Data augmentation 
- Noise robustness : noise를 weight에도 넣어줌.
- Label smooothing : 
- Dropout : 
- Batch normalization

---
- Mixup, CutMix
  - 성능이 정말 많이 올라감.