# 3강 Optimization
## data augmentation
### important Concepts in Optimization
- Generalization
- Under-fitting vs over-fitting
- Cross validation
- Bias-variance tradeoff
- Bootstrapping
- Bagging and boosting

### Generalization
![image](https://user-images.githubusercontent.com/50571795/129368683-6eb8eb33-55fc-4617-a5f7-1d2ff82e9a53.png)

### Underfitting vs Overfitting
![image](https://user-images.githubusercontent.com/50571795/129368770-28f13f4f-046c-4524-be29-e2719329f080.png)

### Cross-validation
![image](https://user-images.githubusercontent.com/50571795/129368871-0ae67b6a-d28c-4c35-a831-953e9bc81cdb.png)

### Bias and Variance

![image](https://user-images.githubusercontent.com/50571795/129369026-79665626-1f86-421f-b7af-47e2351c60d9.png)

![image](https://user-images.githubusercontent.com/50571795/129369073-baccbfc8-76d2-4d76-b266-073f00ae8f22.png)

### Bootstrapping
- Bagging(**B**oostrapping **agg**regat**ing**)
  - 여러 개의 모델을 병렬적으로 처리하여 voting을 통해서 결과를 냄
- Boosting
  - 여러 개의 약한 모델을 직렬로 연결하여 강한 모델을 만들어냄.

## Practical Gradient Descent Methods
- Stochastic gradient descent : 한 개의 샘플에 대해서 gradient를 계산하여 업데이트
- Mini-batch gradient descent : 한 부분집합에 대해서 gradient를 계산해서 업데이트 우리가 흔히 말하는 SGD
- Batch gradient descent : 전체 데이터에 대해서 업데이트. 느림. 

![image](https://user-images.githubusercontent.com/50571795/129373308-4887ffa8-9103-4644-bd94-9f6480e738eb.png)  
우리는 FlatMinimum에 도달하는 것이 좋다. generalize performance가 좋음.

**Gradient Descent**  
![image](https://user-images.githubusercontent.com/50571795/129373429-f11f221b-8289-449d-862b-620ca6e2ba79.png)
**Momentum**  
![image](https://user-images.githubusercontent.com/50571795/129373535-0bbc22e0-0e74-445f-b39d-9654e0375726.png)  
- Beata라는 hyper parameter를 가지고 있다.
- 그렇게 하여 이전의 gradient를 가지고 있다.

**Nesterov Accelerated Gradient**  
![image](https://user-images.githubusercontent.com/50571795/129373631-f07a0cb3-d1f2-4072-941c-45983b648c5b.png)
- 비슷하지만 약간 컨셉이 다르다.
- lookahead gradient를 가지고 있다.
- 먼저 한번 가보고 거기에서 gradient를 구해서 더한다.
- 이렇게 하게되면 local minimum에 먼저 수렴할 수 있다.  

**Adagrad**  
![image](https://user-images.githubusercontent.com/50571795/129475233-9728c45a-238d-44d9-bd7e-e928b71561ec.png)
- 얼만큼 parameter가 변해왔는지, 
- 많이 변해왔으면 적게 변화, 적게 변화했으면 많이 변화

**Adadelta**  
![image](https://user-images.githubusercontent.com/50571795/129475272-a58fd037-451d-4b93-8dce-9b97f9aa5088.png)
- adagrad에서 g_t가 계속 커지는 것을 막기위해,
- 그리고 parameter를 계속 갖고 있어야하는데, 이를 막을 수 있음.
- learning rate이 없다.(-> 바꿀 수 있는 것이 거의 없고 이는 잘 쓰이지 않는 이유가 됨)
**Adam**  
![image](https://user-images.githubusercontent.com/50571795/129373785-a4252811-e02d-469d-b1c3-f7b5082d14c7.png)

## Regularization
- Early stopping
- Parameter norm penalty
- Data augmentation 
- Noise robustness : noise를 weight에도 넣어줌.
- Label smooothing : 
- Dropout : 
- Batch normalization

---
- Mixup, CutMix
  - 성능이 정말 많이 올라감.

