## data augmentation
### important Concepts in Optimization
- Generalization
- Under-fitting vs over-fitting
- Cross validation
- Bias-variance tradeoff
- Bootstrapping
- Bagging and boosting

### Generalization
![image](https://user-images.githubusercontent.com/50571795/129368683-6eb8eb33-55fc-4617-a5f7-1d2ff82e9a53.png)

### Underfitting vs Overfitting
![image](https://user-images.githubusercontent.com/50571795/129368770-28f13f4f-046c-4524-be29-e2719329f080.png)

### Cross-validation
![image](https://user-images.githubusercontent.com/50571795/129368871-0ae67b6a-d28c-4c35-a831-953e9bc81cdb.png)

### Bias and Variance

![image](https://user-images.githubusercontent.com/50571795/129369026-79665626-1f86-421f-b7af-47e2351c60d9.png)

![image](https://user-images.githubusercontent.com/50571795/129369073-baccbfc8-76d2-4d76-b266-073f00ae8f22.png)

### Bootstrapping
- Bagging(**B**oostrapping **agg**regat**ing**)
  - 여러 개의 모델을 병렬적으로 처리하여 voting을 통해서 결과를 냄
- Boosting
  - 여러 개의 약한 모델을 직렬로 연결하여 강한 모델을 만들어냄.

## Practical Gradient Descent Methods
- Stochastic gradient descent : 한 개의 샘플에 대해서 gradient를 계산하여 업데이트
- Mini-batch gradient descent : 한 부분집합에 대해서 gradient를 계산해서 업데이트 우리가 흔히 말하는 SGD
- Batch gradient descent : 전체 데이터에 대해서 업데이트. 느림.

### Cross-validation

- Mixup, CutMix
  - 성능이 정말 많이 올라감.