# 8강 transformer
- Sequential Data를 다루는 방법론
- 이번 강의는 transformer에 대해서만 다루게 됨.

transformer는 sequential data를 처리하고 인코딩 하는 방법론이기 때문에, 다양한 분야에서 적용할 수 있다.(ex. 이미지 분류, 등등)

1. N개의 단어가 어떻게 한번에 encoding이 되는가?
2. encoder와 decoder가 어떻게 정보를 주고 받는가?
3. decoder가 어떻게 generation할 수 있는가?


### encoder
![image](https://user-images.githubusercontent.com/50571795/129484648-8e40c65e-054d-4d5e-99b8-fbf6645ac5ba.png)
#### self-Attention
- 하나의 백터가 feed Forward로 넘어갈 때, 나머지 백터를 참고해서 만들게 됨.
- 따라서 self-Attention에서는 백터간의 의존성이 존재함.
#### Feed Forward Neural Network
- Feed forward에서는 

![image](https://user-images.githubusercontent.com/50571795/129484820-785346d2-9f74-45a4-8d5a-4c7a273cb32c.png)
