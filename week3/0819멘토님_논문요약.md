# ViT visual transformer
- nlp task에서 잘하고 있다.
- standard가 되어가고 있다.(이 말은 정말 엄청난 말이다. 표준이라고 얘기하는 것)
- 비전에서는 어려웠으나 해결했다
- 이미지 패치를 넣었다.
-

## Introduction
씨엔엔 구조나, 어텐션으로는 레즈넷 만큼 성능이 나오지 못한다. 왜 그럴까?
컴바인을 한 것이 문제?, standard transformer를 사용해볼까?
우리는 2차원 이미지를 갖고 있음, 이는 patch로 만들어서 sequence로 넣어서 마치 word처럼 보고 있다.
mid-sized data에서는 성능이 엄청 좋지는 않았다. 하지만 larger dataset에서는 sota가 되었다.
inductive bias 때문에 데이터 사이즈에 따라서 성능이 달라지는 것이 아닐까?
inductive bias는 간단하게 생각하면 추가적 가정
ex) 사람이 공을 던졌다.우리는 공의 궤적을 알고 있음. 그리고 궤적만 가지고 공이 어떤 공인지 맞춰야 한다.
여기에 공의 크기까지 안다면? 좀 더 맞추기가 쉬워진다.  

여기서는 inductive bias가 어떻게 쓰였는가? ViT는 cnn(locality), rnn(sqeunce)라는 가정이 없음.
대신 데이터가 정말 많아진다면, 공의 크기가 오히려 방해가 되지 않을까? 

트렌스포머와 다른부분 딱 한 가지, MLP와 Norm의 위치를 바꿨다. 이는 이미지에서 유의미한 행동이다.

2D-conv-flatten 된 것이 기존의 embedding vector가 word를 대체한다.

이미지를 word처럼 봤는데 성능이 잘 나왔다.

# AAE

## Abstract
여러가지로 쓰일 수 있다. style transfer, semi-supervised 등 다양하게 쓰일 수 있다.

## Introduction
MCMC sampling을 잘, 많이, 열심히 모분포를 잘 근사해보자.
VAE와 GAN의 장점을 살려보자.
encoder와 decoder를 통해서

