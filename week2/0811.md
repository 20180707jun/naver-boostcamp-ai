# 4강 Convolution은 무엇인가?

- dense layer는 mlp를 의미한다.(따라서 파라미터가 급증한다.)

# 5강 modern cnn - 1x1 convolution의 중요성

- ILSVRC
  - 연구자 본인이 training하고 test해봤는데 5.1%의 error rate이 나왔다는 후문이 있다.

### AlexNet
- ReLU 사용해서 성능이 좋아짐.
  - 줄어들어서 베니싱확률 적음
  - nonlinear함.
  - data augmentation
- 베니싱 문제를 극복했다.

### VGGNet
- 3x3을 활용했다.
  - 3x3=9, 5x5=25 따라서 3x3은 parameter수를 굉장히 줄일 수 있다.

### GoogleNet
- 1x1 Conv가 들어가는데,
  - parameter 수를 굉장히 줄일 수 있고,
  - concat하는 성질도 좋은 성질.
  - 채널방향으로 차원 축소 가능.  

![image](https://user-images.githubusercontent.com/50571795/129098851-cbcce8f0-0e38-4ba3-9eca-71439b2998ea.png)

### ResNet
- 기존에는 네트워크가 커지면서 학습이 안되는 것이 발생한다.
- network를 더 깊게 쌓을 수 있는 가능성을 열어주 었다.
  - residual을 이용해서.
- bottlenect archtecture
  - 1x1 conv하는 것.  

![image](https://user-images.githubusercontent.com/50571795/129099413-d3f86ee6-94cb-4c96-b6f0-81552b92d978.png)

### DenseNet
-  \+ 대신 concat을 한다. 
- 마지막에 1x1conv로 합친다.
- 분류에서는 결과가 굉장히 잘나옴(분류는 ResNet or DenseNet)
